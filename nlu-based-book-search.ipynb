{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLU based item search¶\n",
    "Using a pretrained BERT and Elasticsearch KNN to search textually similar items\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Background\n",
    "2. Setup\n",
    "3. Data Preparation \n",
    "4. SageMaker Model Hosting\n",
    "5. Build a KNN Index in Elasticsearch\n",
    "6. Evaluate Index Search Results\n",
    "7. Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "In this notebook, we'll build the core components of a textually similar items search. Sometime people don't know what exactly they are looking in that case they just type an item descriptions and it will retrive the similar items.\n",
    "\n",
    "One of the core components of textually similar items search is a fixed length sentence/word embedding i,e a “feature vectors” corrosponds to that text. The reference word/sentence embedding typically are generated offline and must be stored in. So they can be efficiently searched. So generating word/sengtence embedding can be achived by pretrained language model such as BRET(Bidirectional Encoder Representations from Transformers). In our use case we have used pretrained BERT model from sentence-transformers(https://github.com/UKPLab/sentence-transformers).\n",
    "\n",
    "To enable efficient searches for textually similar items, we'll use Amazon SageMaker to generate fixed length sentence embedding i.e “feature vectors” and use KNN algorithim in Amazon Elasticsearch service. KNN for Amazon Elasticsearch Service7.7 lets you search for points in a vector space and find the \"nearest neighbors\" for those points by cosine similarity (Default is Euclidean distance). Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "Here are the steps we'll follow to build textually similar items: After some initial setup, we'll host the pretrained BERT language model in SageMaker PyTorch model server. Then generate feature vectors for Multi-modal Corpus of Fashion Images from feidegger, a zalandoresearch dataset. Those feature vectors will be imported in Amazon Elasticsearch KNN Index. Next, we'll explore some sample text queries, and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install tqdm to have progress bar\n",
    "!pip install tqdm\n",
    "\n",
    "#install necessary pkg to make connection with elasticsearch domain\n",
    "!pip install elasticsearch\n",
    "!pip install requests\n",
    "!pip install requests-aws4auth\n",
    "!pip install \"sagemaker<2.0\" --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "def get_cfn_outputs(stackname):\n",
    "    outputs = {}\n",
    "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
    "        outputs[output['OutputKey']] = output['OutputValue']\n",
    "    return outputs\n",
    "\n",
    "## Setup variables to use for the rest of the demo\n",
    "cloudformation_stack_name = \"ml-search-stack\"\n",
    "\n",
    "outputs = get_cfn_outputs(cloudformation_stack_name)\n",
    "\n",
    "bucket = outputs['s3BucketTraining']\n",
    "es_host = outputs['esHostName']\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation ¶\n",
    "The dataset itself consists of books with a long description and image.\n",
    "\n",
    "Downloading Books data: Data originally from here: https://github.com/dris1995/deep-learning-semantic-search-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "import json\n",
    "import tqdm\n",
    "import urllib.request\n",
    "from tqdm import notebook\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "images_path = 'data/books/'\n",
    "filename = 'metadata.json'\n",
    "my_bucket = s3_resource.Bucket(bucket)\n",
    "clean_data = []\n",
    "\n",
    "if not os.path.isdir(images_path):\n",
    "    os.makedirs(images_path)\n",
    "\n",
    "def download_metadata(url):\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        \n",
    "#download metadata.json to local notebook\n",
    "download_metadata('https://raw.githubusercontent.com/dris1995/deep-learning-semantic-search-engine/main/books.json')\n",
    "\n",
    "\n",
    "def generate_image_list(filename):\n",
    "    metadata = open(filename,'r')\n",
    "    data = json.load(metadata)\n",
    "    url_lst = []\n",
    "    non_desc_count = 0\n",
    "    total_count = len(data)\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if 'longDescription' not in data[i].keys() or 'thumbnailUrl' not in data[i].keys():\n",
    "            non_desc_count +=1\n",
    "            continue\n",
    "        else:\n",
    "            url_lst.append(data[i]['thumbnailUrl'])\n",
    "            clean_data.append(data[i])\n",
    "    print(f'Total count data-set: {total_count}') \n",
    "    print(f'Total count not desc: {non_desc_count}')\n",
    "    print(f'Total count to go in data-set:{len(url_lst)}')\n",
    "    return url_lst\n",
    "\n",
    "\n",
    "def download_image(url):\n",
    "   urllib.request.urlretrieve(url, images_path + '/' + url.split(\"/\")[-1])\n",
    "                    \n",
    "#generate image list            \n",
    "url_lst = generate_image_list(filename)     \n",
    "\n",
    "workers = 2 * cpu_count()\n",
    "\n",
    "#downloading images to local disk\n",
    "process_map(download_image, url_lst, max_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uploading dataset to S3\n",
    "files_to_upload = []\n",
    "dirName = 'data'\n",
    "for path, subdirs, files in os.walk('./' + dirName):\n",
    "    path = path.replace(\"\\\\\",\"/\")\n",
    "    directory_name = path.replace('./',\"\")\n",
    "    for file in files:\n",
    "        files_to_upload.append({\n",
    "            \"filename\": os.path.join(path, file),\n",
    "            \"key\": directory_name+'/'+file\n",
    "        })\n",
    "        \n",
    "\n",
    "def upload_to_s3(file):\n",
    "        my_bucket.upload_file(file['filename'], file['key'])\n",
    "        \n",
    "#uploading images to s3\n",
    "process_map(upload_to_s3, files_to_upload, max_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lauange Translation Results\n",
    "This dataset has book descriptions in English. We will save to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define description result function\n",
    "results = []\n",
    "def get_descriptions(data):\n",
    "    results = []\n",
    "    for i in range(len(data)):\n",
    "        trim_name = f's3://{bucket}/data/books/' + data[i]['thumbnailUrl'].split(\"/\")[-1]\n",
    "        data_to_add = dict(filename=trim_name,descriptions=data[i]['longDescription'],title=data[i]['title'])\n",
    "        results.append(data_to_add)\n",
    "    return results\n",
    "\n",
    "# def get_descriptions(data):\n",
    "#     results = {}\n",
    "#     results['filename'] = f's3://{bucket}/data/books/' + data['thumbnailUrl'].split(\"/\")[-1]\n",
    "#     results['descriptions'] = []\n",
    "#     for i in data['longDescription']:\n",
    "#         results['descriptions'].append(data[i]['longDescription'])\n",
    "#     print(results)   \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_descriptions(clean_data)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the translated text in json format in case you need later time\n",
    "with open('books-clean-data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model Hosting\n",
    "In this section will host the pretrained BERT model into SageMaker Pytorch model server to generte 768 fixed length sentecce embedding from sentence-transformers (https://github.com/UKPLab/sentence-transformers).\n",
    "\n",
    "Citation:\n",
    "@inproceedings{reimers-2019-sentence-bert,\n",
    "title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\n",
    "author = \"Reimers, Nils and Gurevych, Iryna\",\n",
    "booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\n",
    "month = \"11\",\n",
    "year = \"2019\",\n",
    "publisher = \"Association for Computational Linguistics\",\n",
    "url = \"http://arxiv.org/abs/1908.10084\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install sagemaker_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#Save the model to disk which we will host at sagemaker\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "saved_model_dir = 'transformer'\n",
    "if not os.path.isdir(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "model.save(saved_model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining defalut bucket for SageMaker pretrained model hosting\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip the model .gz format\n",
    "import tarfile\n",
    "export_dir = 'transformer'\n",
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add(export_dir, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upload the model to S3\n",
    "inputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a PyTorchModel object. The deploy() method on the model object creates an endpoint which serves prediction requests in real-time. If the instance_type is set to a SageMaker instance type (e.g. ml.m5.large) then the model will be deployed on SageMaker. If the instance_type parameter is set to local then it will be deployed locally as a Docker container and ready for testing locally.\n",
    "\n",
    "First we need to create a RealTimePredictor class to accept TEXT as input and output JSON. The default behaviour is to accept a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch, PyTorchModel\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "class StringPredictor(RealTimePredictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session):\n",
    "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pytorch_model = PyTorchModel(model_data = inputs, \n",
    "                             role=role, \n",
    "                             entry_point ='inference.py',\n",
    "                             source_dir = './code',\n",
    "                             py_version = 'py3', \n",
    "                             framework_version = '1.5.1',\n",
    "                             predictor_cls=StringPredictor)\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m5.large', initial_instance_count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence transformers uses BERT pretrained model so it will generate 768 dimension for the given text. we will quickly validate the same in next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing a quick test to make sure model is generating the embeddings\n",
    "import json\n",
    "import sagemaker_containers\n",
    "payload = 'When it comes to mobile apps, Android can do almost anything'\n",
    "features = predictor.predict(payload)\n",
    "embedding = json.loads(features)\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
